adam.rodrigues@ADAMPC:~$ kubectl config get-contexts
CURRENT   NAME                                                              CLUSTER                                                           AUTHINFO                                                          NAMESPACE
*         arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   
adam.rodrigues@ADAMPC:~$ nano ~/.aws/config
adam.rodrigues@ADAMPC:~$ aws eks list-clusters --region us-east-1 --profile client1-shared

An error occurred (AccessDenied) when calling the AssumeRole operation: User: arn:aws:iam::859581267860:user/adam.rodrigues is not authorized to perform: sts:AssumeRole on resource: arn:aws:iam::999999999999:role/dba
adam.rodrigues@ADAMPC:~$ aws eks list-clusters --region us-east-1 --profile client1-sharedf
{
    "clusters": [
        "client1-shared"
    ]
}
adam.rodrigues@ADAMPC:~$ aws eks update-kubeconfig \
  --region us-east-1 \
  --name client1-shared \
  --profile client1-sharedf
Added new context arn:aws:eks:us-east-1:999999999999:cluster/client1-shared to /home/adam.rodrigues/.kube/config
adam.rodrigues@ADAMPC:~$ kubectl config get-contexts
CURRENT   NAME                                                              CLUSTER                                                           AUTHINFO                                                          NAMESPACE
          arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   
*         arn:aws:eks:us-east-1:999999999999:cluster/client1-shared           arn:aws:eks:us-east-1:999999999999:cluster/client1-shared           arn:aws:eks:us-east-1:999999999999:cluster/client1-shared           
adam.rodrigues@ADAMPC:~$ aws sts get-caller-identity --profile client1-sharedf
{
    "UserId": "XXXXXXXXXXXXXXXXXX:botocore-session-1754325652",
    "Account": "999999999999",
    "Arn": "arn:aws:sts::999999999999:assumed-role/foundation/botocore-session-1754325652"
}
adam.rodrigues@ADAMPC:~$ aws eks update-kubeconfig \
  --region us-east-1 \
  --name client1-shared \
  --profile client1-sharedt
Updated context arn:aws:eks:us-east-1:999999999999:cluster/client1-shared in /home/adam.rodrigues/.kube/config
adam.rodrigues@ADAMPC:~$ kubectl config get-contexts
CURRENT   NAME                                                              CLUSTER                                                           AUTHINFO                                                          NAMESPACE
          arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   arn:aws:eks:us-east-1:888888888888:cluster/client1-data-analytics   
*         arn:aws:eks:us-east-1:999999999999:cluster/client1-shared           arn:aws:eks:us-east-1:999999999999:cluster/client1-shared           arn:aws:eks:us-east-1:999999999999:cluster/client1-shared           
adam.rodrigues@ADAMPC:~$ kubectl get pods -n pmm
No resources found in pmm namespace.
adam.rodrigues@ADAMPC:~$ kubectl get ns
NAME              STATUS   AGE
argocd            Active   615d
default           Active   623d
infra-test        Active   545d
kube-node-lease   Active   623d
kube-public       Active   623d
kube-system       Active   623d
observability     Active   506d
adam.rodrigues@ADAMPC:~$ kubectl get pods -n observability
NAME    READY   STATUS    RESTARTS   AGE
pmm-0   1/1     Running   0          104d
adam.rodrigues@ADAMPC:~$ kubectl exec -it pmm-0 -n observability -- bash
[root@pmm-0 opt] # pmm-admin --version
ProjectName: pmm-admin
Version: 2.42.0
PMMVersion: 2.42.0
Timestamp: 2024-06-06 15:29:35 (UTC)
FullCommit: 74e57527735bd062c4bd37adbd89c31bb14ebc15
[root@pmm-0 opt] # helm list -n observability
bash: helm: command not found
[root@pmm-0 opt] # exit
exit
adam.rodrigues@ADAMPC:~$ curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 11913  100 11913    0     0   144k      0 --:--:-- --:--:-- --:--:--  145k
Downloading https://get.helm.sh/helm-v3.18.4-linux-amd64.tar.gz
Verifying checksum... Done.
Preparing to install helm into /usr/local/bin
[sudo] senha para adam.rodrigues: 
Sinto muito, tente novamente.
[sudo] senha para adam.rodrigues: 
helm installed into /usr/local/bin/helm
adam.rodrigues@ADAMPC:~$ helm version
version.BuildInfo{Version:"v3.18.4", GitCommit:"d80839cf37d860c8aa9a0503fe463278f26cd5e2", GitTreeState:"clean", GoVersion:"go1.24.4"}
adam.rodrigues@ADAMPC:~$ helm list -n observability
NAME	NAMESPACE    	REVISION	UPDATED                                	STATUS  	CHART     	APP VERSION
pmm 	observability	1       	2024-08-22 18:04:08.664096856 -0300 -03	deployed	pmm-1.3.14	2.42.0     
adam.rodrigues@ADAMPC:~$ helm repo add percona https://percona.github.io/percona-helm-charts/
"percona" has been added to your repositories
adam.rodrigues@ADAMPC:~$ helm repo update
Hang tight while we grab the latest from your chart repositories...
...Successfully got an update from the "percona" chart repository
Update Complete. ⎈Happy Helming!⎈
adam.rodrigues@ADAMPC:~$ helm search repo percona/pmm-server
No results found
adam.rodrigues@ADAMPC:~$ helm search repo percona/pmm
NAME       	CHART VERSION	APP VERSION	DESCRIPTION                                       
percona/pmm	1.4.7        	3.3.1      	A Helm chart for Percona Monitoring and Managem...
adam.rodrigues@ADAMPC:~$ helm upgrade pmm percona/pmm \
  --namespace observability \
  -f pmm-values-backup.yaml
Error: open pmm-values-backup.yaml: no such file or directory
adam.rodrigues@ADAMPC:~$ helm get values pmm -n observability -o yaml > pmm-values-backup.yaml
adam.rodrigues@ADAMPC:~$ ls -lh pmm-values-backup.yaml
-rw-rw-r-- 1 adam.rodrigues adam.rodrigues 458 ago  7 10:55 pmm-values-backup.yaml
adam.rodrigues@ADAMPC:~$ helm upgrade pmm percona/pmm \
  --namespace observability \
  -f pmm-values-backup.yaml
Error: UPGRADE FAILED: cannot patch "pmm" with kind StatefulSet: StatefulSet.apps "pmm" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'ordinals', 'template', 'updateStrategy', 'persistentVolumeClaimRetentionPolicy' and 'minReadySeconds' are forbidden
adam.rodrigues@ADAMPC:~$ helm upgrade pmm percona/pmm \
  --namespace observability \
  -f pmm-values-backup.yaml \
  --reuse-values \
  --force
Error: UPGRADE FAILED: failed to replace object: StatefulSet.apps "pmm" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'ordinals', 'template', 'updateStrategy', 'persistentVolumeClaimRetentionPolicy' and 'minReadySeconds' are forbidden
adam.rodrigues@ADAMPC:~$ kubectl delete statefulset pmm -n observability --cascade=orphan
statefulset.apps "pmm" deleted
adam.rodrigues@ADAMPC:~$ helm upgrade pmm percona/pmm \
  --namespace observability \
  -f pmm-values-backup.yaml
Release "pmm" has been upgraded. Happy Helming!
NAME: pmm
LAST DEPLOYED: Thu Aug  7 10:57:08 2025
NAMESPACE: observability
STATUS: deployed
REVISION: 4
TEST SUITE: None
NOTES:
Percona Monitoring and Management (PMM)

An open source database monitoring, observability and management tool
Check more info here: https://docs.percona.com/percona-monitoring-and-management/index.html

Get the application URL:
  export NODE_PORT=$(kubectl get --namespace observability -o jsonpath="{.spec.ports[0].nodePort}" services monitoring-service)
  export NODE_IP=$(kubectl get nodes --namespace observability -o jsonpath="{.items[0].status.addresses[0].address}")
  echo https://$NODE_IP:$NODE_PORT

adam.rodrigues@ADAMPC:~$ kubectget podsmm -n observability
NAME    READY   STATUS    RESTARTS   AGE
pmm-0   0/1     Running   0          9m30s
adam.rodrigues@ADAMPC:~$ kubectl logs pmm-0 -n observability -f
time="2025-08-07T13:57:57.844+00:00" level=warning msg="Configuration warning: unknown environment variable \"DISABLE_UPDATES=1\"."
time="2025-08-07T13:57:57.844+00:00" level=warning msg="Configuration warning: unknown environment variable \"PMM_ADMIN_PASSWORD=P@ssw0rd\"."
time="2025-08-07T13:57:57.845+00:00" level=warning msg="Configuration warning: unknown environment variable \"DATA_RETENTION=720h\"."
2025-08-07 13:57:58,272 INFO Included extra file "/etc/supervisord.d/grafana.ini" during parsing
2025-08-07 13:57:58,273 INFO Included extra file "/etc/supervisord.d/nomad-server.ini" during parsing
2025-08-07 13:57:58,273 INFO Included extra file "/etc/supervisord.d/pmm.ini" during parsing
2025-08-07 13:57:58,273 INFO Included extra file "/etc/supervisord.d/qan-api2.ini" during parsing
2025-08-07 13:57:58,273 INFO Included extra file "/etc/supervisord.d/supervisord.ini" during parsing
2025-08-07 13:57:58,273 INFO Included extra file "/etc/supervisord.d/victoriametrics.ini" during parsing
2025-08-07 13:57:58,273 INFO Included extra file "/etc/supervisord.d/vmalert.ini" during parsing
2025-08-07 13:57:58,273 INFO Included extra file "/etc/supervisord.d/vmproxy.ini" during parsing
2025-08-07 13:57:58,273 INFO Set uid to user 1000 succeeded
2025-08-07 13:57:58,280 INFO RPC interface 'supervisor' initialized
2025-08-07 13:57:58,280 INFO supervisord started with pid 1
2025-08-07 13:57:59,282 INFO spawned: 'pmm-init' with pid 12
2025-08-07 13:57:59,284 INFO spawned: 'postgresql' with pid 13
2025-08-07 13:57:59,286 INFO spawned: 'clickhouse' with pid 14
2025-08-07 13:57:59,288 INFO spawned: 'grafana' with pid 15
2025-08-07 13:57:59,290 INFO spawned: 'nginx' with pid 16
2025-08-07 13:57:59,300 INFO spawned: 'victoriametrics' with pid 17
2025-08-07 13:57:59,304 INFO spawned: 'vmalert' with pid 18
2025-08-07 13:57:59,311 INFO spawned: 'vmproxy' with pid 22
2025-08-07 13:57:59,312 INFO spawned: 'qan-api2' with pid 23
2025-08-07 13:57:59,321 INFO spawned: 'pmm-managed' with pid 24
2025-08-07 13:57:59,367 INFO exited: qan-api2 (exit status 1; not expected)
2025-08-07 13:57:59,427 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:57:59,985 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:00,470 INFO success: pmm-init entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:00,472 INFO spawned: 'postgresql' with pid 54
2025-08-07 13:58:00,472 INFO success: clickhouse entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:00,472 INFO success: nginx entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:00,472 INFO success: victoriametrics entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:00,472 INFO success: vmalert entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:00,473 INFO success: vmproxy entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:00,474 INFO spawned: 'qan-api2' with pid 55
2025-08-07 13:58:00,475 INFO success: pmm-managed entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:00,493 INFO exited: qan-api2 (exit status 1; not expected)
2025-08-07 13:58:00,495 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:01,007 INFO spawned: 'grafana' with pid 71
2025-08-07 13:58:02,126 INFO success: grafana entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:02,127 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:02,189 INFO spawned: 'grafana' with pid 108
2025-08-07 13:58:02,506 INFO spawned: 'postgresql' with pid 127
2025-08-07 13:58:02,507 INFO spawned: 'qan-api2' with pid 128
2025-08-07 13:58:02,565 INFO exited: qan-api2 (exit status 1; not expected)
2025-08-07 13:58:02,574 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:03,201 INFO success: grafana entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:03,484 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:04,486 INFO spawned: 'grafana' with pid 457
2025-08-07 13:58:05,486 INFO success: grafana entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:05,600 INFO spawned: 'postgresql' with pid 465
2025-08-07 13:58:05,654 INFO spawned: 'qan-api2' with pid 466
2025-08-07 13:58:06,127 INFO exited: qan-api2 (exit status 1; not expected)
2025-08-07 13:58:07,249 INFO success: postgresql entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:07,250 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:07,274 INFO spawned: 'postgresql' with pid 506
2025-08-07 13:58:07,495 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:08,163 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:08,494 INFO spawned: 'grafana' with pid 530
2025-08-07 13:58:08,921 INFO spawned: 'postgresql' with pid 537
2025-08-07 13:58:08,937 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:08,938 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:10,495 INFO spawned: 'grafana' with pid 538
2025-08-07 13:58:10,497 INFO spawned: 'qan-api2' with pid 539
2025-08-07 13:58:11,093 INFO spawned: 'postgresql' with pid 549
2025-08-07 13:58:11,107 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:11,109 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:12,494 INFO success: qan-api2 entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:13,496 INFO spawned: 'grafana' with pid 550
2025-08-07 13:58:13,936 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:14,497 INFO spawned: 'postgresql' with pid 556
2025-08-07 13:58:14,519 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:17,251 INFO spawned: 'grafana' with pid 557
2025-08-07 13:58:17,912 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:19,272 INFO spawned: 'postgresql' with pid 563
2025-08-07 13:58:19,319 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:22,501 INFO spawned: 'grafana' with pid 566
2025-08-07 13:58:22,913 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:24,501 INFO spawned: 'postgresql' with pid 573
2025-08-07 13:58:24,514 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:28,164 INFO spawned: 'grafana' with pid 574
2025-08-07 13:58:29,504 INFO success: grafana entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 13:58:29,504 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:29,506 INFO spawned: 'grafana' with pid 581
2025-08-07 13:58:29,907 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:31,306 INFO spawned: 'postgresql' with pid 587
2025-08-07 13:58:31,308 INFO spawned: 'grafana' with pid 588
2025-08-07 13:58:31,353 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:31,715 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:34,509 INFO spawned: 'grafana' with pid 595
2025-08-07 13:58:34,892 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:38,164 INFO spawned: 'grafana' with pid 602
2025-08-07 13:58:38,512 INFO spawned: 'postgresql' with pid 608
2025-08-07 13:58:38,533 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:38,557 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:43,522 INFO spawned: 'grafana' with pid 609
2025-08-07 13:58:43,949 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:47,251 INFO spawned: 'postgresql' with pid 615
2025-08-07 13:58:47,265 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:49,533 INFO spawned: 'grafana' with pid 616
2025-08-07 13:58:49,928 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:58:56,306 INFO spawned: 'postgresql' with pid 622
2025-08-07 13:58:56,308 INFO spawned: 'grafana' with pid 623
2025-08-07 13:58:56,543 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:58:56,829 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:59:04,549 INFO spawned: 'grafana' with pid 630
2025-08-07 13:59:04,940 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:59:06,552 INFO spawned: 'postgresql' with pid 636
2025-08-07 13:59:06,566 INFO exited: postgresql (exit status 1; not expected)
2025-08-07 13:59:07,250 INFO gave up: postgresql entered FATAL state, too many start retries too quickly
2025-08-07 13:59:13,555 INFO spawned: 'grafana' with pid 637
2025-08-07 13:59:14,065 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:59:23,562 INFO spawned: 'grafana' with pid 644
2025-08-07 13:59:23,941 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:59:34,571 INFO spawned: 'grafana' with pid 651
2025-08-07 13:59:34,959 INFO exited: grafana (exit status 1; not expected)
2025-08-07 13:59:35,571 INFO gave up: grafana entered FATAL state, too many start retries too quickly
2025-08-07 14:00:38,073 INFO exited: pmm-init (exit status 2; not expected)
2025-08-07 14:00:38,164 INFO spawned: 'pmm-init' with pid 662
2025-08-07 14:00:39,626 INFO success: pmm-init entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 14:02:59,754 INFO exited: pmm-managed (exit status 1; not expected)
2025-08-07 14:03:00,528 INFO spawned: 'pmm-managed' with pid 751
2025-08-07 14:03:01,572 INFO success: pmm-managed entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 14:03:10,962 INFO exited: pmm-init (exit status 2; not expected)
2025-08-07 14:03:11,305 INFO spawned: 'pmm-init' with pid 760
2025-08-07 14:03:12,562 INFO success: pmm-init entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2025-08-07 14:05:43,955 INFO exited: pmm-init (exit status 2; not expected)
2025-08-07 14:05:44,704 INFO spawned: 'pmm-init' with pid 852
2025-08-07 14:05:45,704 INFO success: pmm-init entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)


adam.rodrigues@ADAMPC:~$ kubectl get pods -n observability
NAME    READY   STATUS    RESTARTS   AGE
pmm-0   0/1     Running   0          14m
adam.rodrigues@ADAMPC:~$ kubectl get pods -n observability
NAME    READY   STATUS    RESTARTS   AGE
pmm-0   0/1     Running   0          22m
adam.rodrigues@ADAMPC:~$ kubectl exec -it pmm-0 -n observability -- bash
[pmm@pmm-0 opt] # tail -n 100 -f /srv/logs/pmm-managed.log
time="2025-08-07T14:19:49.990+00:00" level=info msg="Migrating database..." component=migration
time="2025-08-07T14:19:49.990+00:00" level=warning msg="Failed to migrate database: dial tcp 127.0.0.1:5432: connect: connection refused." component=migration
time="2025-08-07T14:19:50.990+00:00" level=info msg="Migrating database..." component=migration
time="2025-08-07T14:19:50.991+00:00" level=warning msg="Failed to migrate database: dial tcp 127.0.0.1:5432: connect: connection refused." component=migration
time="2025-08-07T14:19:51.992+00:00" level=info msg="Migrating database..." component=migration
......

[pmm@pmm-0 opt] # supervisorctl status
clickhouse                       RUNNING   pid 14, uptime 0:29:22
grafana                          FATAL     Exited too quickly (process log may have details)
nginx                            RUNNING   pid 16, uptime 0:29:22
nomad-server                     STOPPED   Not started
pmm-agent                        STOPPED   Not started
pmm-init                         RUNNING   pid 1692, uptime 0:01:14
pmm-managed                      RUNNING   pid 1591, uptime 0:04:16
postgresql                       FATAL     Exited too quickly (process log may have details)
qan-api2                         RUNNING   pid 539, uptime 0:29:11
victoriametrics                  RUNNING   pid 17, uptime 0:29:22
vmalert                          RUNNING   pid 18, uptime 0:29:22
vmproxy                          RUNNING   pid 22, uptime 0:29:22
[pmm@pmm-0 opt] # tail -n 200 -f /srv/logs/postgresql.log
tail: cannot open '/srv/logs/postgresql.log' for reading: No such file or directory
tail: no files remaining
[pmm@pmm-0 opt] # ls -l /srv/logs/
total 580252
-rw-rw-r--. 1 root pmm    44151 Aug  7 13:57 alertmanager.log
-rw-rw-r--. 1 root pmm  4649826 Aug  7 13:58 clickhouse-server.log
-rw-rw-r--. 1 root pmm  9045184 Aug  7 13:59 grafana.log
-rw-rw-r--. 1 root pmm 52428986 Aug  7 00:43 grafana.log.1
-rw-rw-r--. 1 root pmm 52428804 Aug  3 19:41 grafana.log.2
-rw-rw-r--. 1 root pmm 41930738 Aug  7 14:29 nginx.log
-rw-rw-r--. 1 root pmm 52428825 Jul 31 13:45 nginx.log.1
-rw-rw-r--. 1 root pmm 52428858 Jul 24 09:14 nginx.log.2
-rw-rw-r--. 1 root pmm 27733105 Aug  7 13:57 pmm-agent.log
-rw-rw-r--. 1 root pmm 52429259 Aug  7 03:03 pmm-agent.log.1
-rw-rw-r--. 1 root pmm 52428913 Aug  6 06:27 pmm-agent.log.2
-rw-r--r--. 1 pmm  pmm    23985 Aug  7 14:28 pmm-init.log
-rw-rw-r--. 1 root pmm 18668491 Aug  7 14:29 pmm-managed.log
-rw-rw-r--. 1 root pmm 52428875 Aug  6 22:24 pmm-managed.log.1
-rw-rw-r--. 1 root pmm 52428822 Aug  5 01:42 pmm-managed.log.2
-rw-rw-r--. 1 root pmm   134644 Apr 25 07:12 pmm-update-perform-init.log
-rw-rw-r--. 1 root pmm    35274 Aug  7 13:59 postgresql14.log
-rw-rw-r--. 1 root pmm  4363830 Aug  7 13:58 qan-api2.log
-rw-rw-r--. 1 root pmm 10485958 Aug  5 10:55 qan-api2.log.1
-rw-rw-r--. 1 root pmm 10485839 Jul 31 05:32 qan-api2.log.2
-rw-rw-r--. 1 root pmm 10486006 Jul 26 00:07 qan-api2.log.3
-rw-rw-r--. 1 root pmm   143468 Aug  7 14:28 supervisord.log
-rw-rw-r--. 1 root pmm  1176820 Aug  7 14:29 victoriametrics.log
-rw-rw-r--. 1 root pmm   124614 Aug  7 13:58 vmalert.log
-rw-rw-r--. 1 root pmm  3559905 Aug  7 13:57 vmproxy.log
-rw-rw-r--. 1 root pmm 10485788 Jul  7 19:15 vmproxy.log.1
-rw-rw-r--. 1 root pmm 10485763 Jan 17  2025 vmproxy.log.2
-rw-rw-r--. 1 root pmm 10485790 Nov  7  2024 vmproxy.log.3
[pmm@pmm-0 opt] # tail -n 200 -f /srv/logs/postgresql14.log
2025-01-21 14:18:22.420 UTC [13] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-01-21 14:18:22.420 UTC [13] LOG:  listening on IPv6 address "::1", port 5432
2025-01-21 14:18:22.420 UTC [13] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-01-21 14:18:22.425 UTC [13] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-01-21 14:18:22.429 UTC [13] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-01-21 14:18:22.458 UTC [63] LOG:  database system was shut down at 2025-01-21 14:09:51 UTC
2025-01-21 14:18:22.491 UTC [13] LOG:  database system is ready to accept connections
2025-01-21 20:03:59.678 UTC [13] LOG:  received fast shutdown request
2025-01-21 20:03:59.689 UTC [13] LOG:  aborting any active transactions
2025-01-21 20:03:59.693 UTC [13] LOG:  background worker "logical replication launcher" (PID 69) exited with exit code 1
2025-01-21 20:03:59.693 UTC [64] LOG:  shutting down
2025-01-21 20:03:59.715 UTC [13] LOG:  database system is shut down
2025-01-21 20:04:46.769 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-01-21 20:04:46.770 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-01-21 20:04:46.770 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-01-21 20:04:46.779 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-01-21 20:04:46.784 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-01-21 20:04:46.811 UTC [62] LOG:  database system was shut down at 2025-01-21 20:03:59 UTC
2025-01-21 20:04:46.845 UTC [12] LOG:  database system is ready to accept connections
2025-01-21 20:09:10.894 UTC [12] LOG:  received fast shutdown request
2025-01-21 20:09:10.902 UTC [12] LOG:  aborting any active transactions
2025-01-21 20:09:10.905 UTC [12] LOG:  background worker "logical replication launcher" (PID 68) exited with exit code 1
2025-01-21 20:09:10.906 UTC [63] LOG:  shutting down
2025-01-21 20:09:10.938 UTC [12] LOG:  database system is shut down
2025-01-21 20:16:20.503 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-01-21 20:16:20.505 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-01-21 20:16:20.505 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-01-21 20:16:20.553 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-01-21 20:16:20.567 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-01-21 20:16:20.604 UTC [64] LOG:  database system was shut down at 2025-01-21 20:09:10 UTC
2025-01-21 20:16:20.655 UTC [12] LOG:  database system is ready to accept connections
2025-02-13 13:43:12.158 UTC [12] LOG:  received fast shutdown request
2025-02-13 13:43:12.166 UTC [12] LOG:  aborting any active transactions
2025-02-13 13:43:12.169 UTC [12] LOG:  background worker "logical replication launcher" (PID 70) exited with exit code 1
2025-02-13 13:43:12.169 UTC [65] LOG:  shutting down
2025-02-13 13:43:12.189 UTC [12] LOG:  database system is shut down
2025-02-13 14:30:31.793 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-02-13 14:30:31.797 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-02-13 14:30:31.797 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-02-13 14:30:31.828 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-02-13 14:30:31.833 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-02-13 14:30:31.869 UTC [74] LOG:  database system was shut down at 2025-02-13 13:43:12 UTC
2025-02-13 14:30:31.906 UTC [12] LOG:  database system is ready to accept connections
2025-03-20 21:34:13.360 UTC [12] LOG:  received fast shutdown request
2025-03-20 21:34:13.368 UTC [12] LOG:  aborting any active transactions
2025-03-20 21:34:13.372 UTC [12] LOG:  background worker "logical replication launcher" (PID 80) exited with exit code 1
2025-03-20 21:34:13.373 UTC [75] LOG:  shutting down
2025-03-20 21:34:13.395 UTC [12] LOG:  database system is shut down
2025-03-20 21:39:37.956 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-03-20 21:39:37.956 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-03-20 21:39:37.956 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-03-20 21:39:37.962 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-03-20 21:39:37.967 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-03-20 21:39:38.001 UTC [64] LOG:  database system was shut down at 2025-03-20 21:34:13 UTC
2025-03-20 21:39:38.025 UTC [12] LOG:  database system is ready to accept connections
2025-03-26 12:48:43.085 UTC [12] LOG:  received fast shutdown request
2025-03-26 12:48:43.097 UTC [12] LOG:  aborting any active transactions
2025-03-26 12:48:43.099 UTC [12] LOG:  background worker "logical replication launcher" (PID 70) exited with exit code 1
2025-03-26 12:48:43.101 UTC [65] LOG:  shutting down
2025-03-26 12:48:43.115 UTC [12] LOG:  database system is shut down
2025-03-26 12:49:49.737 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-03-26 12:49:49.737 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-03-26 12:49:49.737 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-03-26 12:49:49.825 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-03-26 12:49:49.835 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-03-26 12:49:49.855 UTC [251] LOG:  database system was shut down at 2025-03-26 12:48:43 UTC
2025-03-26 12:49:49.918 UTC [12] LOG:  database system is ready to accept connections
2025-03-26 12:59:58.891 UTC [12] LOG:  received fast shutdown request
2025-03-26 12:59:58.895 UTC [12] LOG:  aborting any active transactions
2025-03-26 12:59:58.898 UTC [12] LOG:  background worker "logical replication launcher" (PID 318) exited with exit code 1
2025-03-26 12:59:58.899 UTC [313] LOG:  shutting down
2025-03-26 12:59:58.924 UTC [12] LOG:  database system is shut down
2025-03-28 13:12:24.387 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-03-28 13:12:24.389 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-03-28 13:12:24.389 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-03-28 13:12:24.403 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-03-28 13:12:24.409 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-03-28 13:12:24.444 UTC [62] LOG:  database system was shut down at 2025-03-26 12:59:58 UTC
2025-03-28 13:12:24.499 UTC [12] LOG:  database system is ready to accept connections
2025-03-28 15:19:41.949 UTC [11] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-03-28 15:19:41.953 UTC [11] LOG:  listening on IPv6 address "::1", port 5432
2025-03-28 15:19:41.953 UTC [11] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-03-28 15:19:41.972 UTC [11] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-03-28 15:19:41.978 UTC [11] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-03-28 15:19:42.025 UTC [65] LOG:  database system was interrupted; last known up at 2025-03-28 13:12:24 UTC
2025-03-28 15:19:42.035 UTC [66] FATAL:  the database system is starting up
2025-03-28 15:19:42.233 UTC [65] LOG:  database system was not properly shut down; automatic recovery in progress
2025-03-28 15:19:42.239 UTC [65] LOG:  redo starts at 0/66FCD120
2025-03-28 15:19:42.251 UTC [65] LOG:  invalid record length at 0/6700AF58: wanted 24, got 0
2025-03-28 15:19:42.251 UTC [65] LOG:  redo done at 0/6700AF20 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.01 s
2025-03-28 15:19:42.297 UTC [11] LOG:  database system is ready to accept connections
2025-03-28 15:42:03.496 UTC [11] LOG:  received fast shutdown request
2025-03-28 15:42:03.505 UTC [11] LOG:  aborting any active transactions
2025-03-28 15:42:03.507 UTC [11] LOG:  background worker "logical replication launcher" (PID 81) exited with exit code 1
2025-03-28 15:42:03.508 UTC [76] LOG:  shutting down
2025-03-28 15:42:03.542 UTC [11] LOG:  database system is shut down
2025-03-28 15:48:44.422 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-03-28 15:48:44.425 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-03-28 15:48:44.425 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-03-28 15:48:44.435 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-03-28 15:48:44.441 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-03-28 15:48:44.461 UTC [64] LOG:  database system was shut down at 2025-03-28 15:42:03 UTC
2025-03-28 15:48:44.490 UTC [12] LOG:  database system is ready to accept connections
2025-03-28 15:49:19.829 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-03-28 15:49:19.829 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-03-28 15:49:19.829 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-03-28 15:49:19.835 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-03-28 15:49:19.840 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-03-28 15:49:19.888 UTC [66] LOG:  database system was interrupted; last known up at 2025-03-28 15:48:44 UTC
2025-03-28 15:49:19.890 UTC [67] FATAL:  the database system is starting up
2025-03-28 15:49:20.064 UTC [66] LOG:  database system was not properly shut down; automatic recovery in progress
2025-03-28 15:49:20.069 UTC [66] LOG:  redo starts at 0/67044768
2025-03-28 15:49:20.083 UTC [66] LOG:  invalid record length at 0/6704E608: wanted 24, got 0
2025-03-28 15:49:20.083 UTC [66] LOG:  redo done at 0/6704E5D0 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.01 s
2025-03-28 15:49:20.089 UTC [73] FATAL:  the database system is starting up
2025-03-28 15:49:20.134 UTC [12] LOG:  database system is ready to accept connections
2025-03-28 16:07:30.857 UTC [12] LOG:  received fast shutdown request
2025-03-28 16:07:30.863 UTC [12] LOG:  aborting any active transactions
2025-03-28 16:07:30.867 UTC [12] LOG:  background worker "logical replication launcher" (PID 79) exited with exit code 1
2025-03-28 16:07:30.867 UTC [74] LOG:  shutting down
2025-03-28 16:07:30.890 UTC [12] LOG:  database system is shut down
2025-03-28 16:13:38.820 UTC [12] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-03-28 16:13:38.821 UTC [12] LOG:  listening on IPv6 address "::1", port 5432
2025-03-28 16:13:38.821 UTC [12] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-03-28 16:13:38.827 UTC [12] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-03-28 16:13:38.831 UTC [12] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-03-28 16:13:38.845 UTC [64] LOG:  database system was shut down at 2025-03-28 16:07:30 UTC
2025-03-28 16:13:38.896 UTC [12] LOG:  database system is ready to accept connections
2025-04-23 18:19:44.437 UTC [1039868] LOG:  PID 1013629 in cancel request did not match any process
2025-04-25 07:10:35.473 UTC [12] LOG:  received fast shutdown request
2025-04-25 07:10:35.481 UTC [12] LOG:  aborting any active transactions
2025-04-25 07:10:35.484 UTC [12] LOG:  background worker "logical replication launcher" (PID 70) exited with exit code 1
2025-04-25 07:10:35.485 UTC [65] LOG:  shutting down
2025-04-25 07:10:35.506 UTC [12] LOG:  database system is shut down
2025-04-25 07:11:48.891 UTC [13] LOG:  starting PostgreSQL 14.11 - Percona Distribution on x86_64-pc-linux-gnu, compiled by gcc (GCC) 11.4.1 20230605 (Red Hat 11.4.1-2.1.0.1), 64-bit
2025-04-25 07:11:48.894 UTC [13] LOG:  listening on IPv6 address "::1", port 5432
2025-04-25 07:11:48.895 UTC [13] LOG:  listening on IPv4 address "127.0.0.1", port 5432
2025-04-25 07:11:48.910 UTC [13] LOG:  listening on Unix socket "/run/postgresql/.s.PGSQL.5432"
2025-04-25 07:11:48.915 UTC [13] LOG:  listening on Unix socket "/tmp/.s.PGSQL.5432"
2025-04-25 07:11:48.933 UTC [64] LOG:  database system was shut down at 2025-04-25 07:10:35 UTC
2025-04-25 07:11:48.962 UTC [13] LOG:  database system is ready to accept connections
2025-06-30 11:06:04.807 UTC [707261] ERROR:  canceling statement due to user request
2025-06-30 11:06:04.807 UTC [707261] STATEMENT:  
			SELECT
				permission.action,
				permission.scope
				FROM permission
				INNER JOIN role ON role.id = permission.role_id
			INNER JOIN (
				SELECT br.role_id FROM builtin_role AS br
				WHERE br.role IN ($1)
				AND (br.org_id = $2 OR br.org_id = $3)
			) as all_role ON role.id = all_role.role_id WHERE role.name LIKE $4
2025-06-30 11:06:05.009 UTC [707263] ERROR:  canceling statement due to user request
2025-06-30 11:06:05.009 UTC [707263] STATEMENT:  
			SELECT
				permission.action,
				permission.scope
				FROM permission
				INNER JOIN role ON role.id = permission.role_id
			INNER JOIN (
				SELECT br.role_id FROM builtin_role AS br
				WHERE br.role IN ($1)
				AND (br.org_id = $2 OR br.org_id = $3)
			) as all_role ON role.id = all_role.role_id WHERE role.name LIKE $4
2025-08-07 13:57:15.629 UTC [13] LOG:  received fast shutdown request
2025-08-07 13:57:15.637 UTC [13] LOG:  aborting any active transactions
2025-08-07 13:57:15.639 UTC [13] LOG:  background worker "logical replication launcher" (PID 70) exited with exit code 1
2025-08-07 13:57:15.641 UTC [65] LOG:  shutting down
2025-08-07 13:57:15.662 UTC [13] LOG:  database system is shut down
2025-08-07 13:57:59.406 UTC [13] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:57:59.406 UTC [13] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:00.493 UTC [54] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:00.493 UTC [54] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:02.563 UTC [127] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:02.563 UTC [127] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:06.171 UTC [465] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:06.171 UTC [465] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:07.317 UTC [506] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:07.317 UTC [506] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:08.937 UTC [537] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:08.937 UTC [537] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:11.108 UTC [549] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:11.108 UTC [549] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:14.517 UTC [556] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:14.517 UTC [556] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:19.318 UTC [563] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:19.318 UTC [563] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:24.513 UTC [573] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:24.513 UTC [573] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:31.351 UTC [587] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:31.351 UTC [587] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:38.530 UTC [608] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:38.530 UTC [608] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:47.264 UTC [615] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:47.264 UTC [615] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:58:56.347 UTC [622] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:58:56.347 UTC [622] HINT:  The server must be started by the user that owns the data directory.
2025-08-07 13:59:06.565 UTC [636] FATAL:  data directory "/srv/postgres14" has wrong ownership
2025-08-07 13:59:06.565 UTC [636] HINT:  The server must be started by the user that owns the data directory.
^C
[pmm@pmm-0 opt] # kubectl get statefulset pmm -n observability -o yaml > pmm-statefulset.yaml
bash: pmm-statefulset.yaml: Permission denied
[pmm@pmm-0 opt] # supervisorctl status
clickhouse                       RUNNING   pid 14, uptime 0:40:20
grafana                          FATAL     Exited too quickly (process log may have details)
nginx                            RUNNING   pid 16, uptime 0:40:20
nomad-server                     STOPPED   Not started
pmm-agent                        STOPPED   Not started
pmm-init                         RUNNING   pid 2089, uptime 0:02:00
pmm-managed                      RUNNING   pid 2177, uptime 0:00:11
postgresql                       FATAL     Exited too quickly (process log may have details)
qan-api2                         RUNNING   pid 539, uptime 0:40:09
victoriametrics                  RUNNING   pid 17, uptime 0:40:20
vmalert                          RUNNING   pid 18, uptime 0:40:20
vmproxy                          RUNNING   pid 22, uptime 0:40:20
[pmm@pmm-0 opt] # ls -la /opt/data/
ls: cannot access '/opt/data/': No such file or directory
[pmm@pmm-0 opt] # cat /srv/logs/postgresql.log | tail -n 30
cat: /srv/logs/postgresql.log: No such file or directory
[pmm@pmm-0 opt] # command terminated with exit code 137
adam.rodrigues@ADAMPC:~$ kubectl exec -it pmm-0 -n observability -- bash
[pmm@pmm-0 opt] # cat /srv/logs/grafana.log | grep admin
logger=context userId=1 orgId=1 uname=admin t=2025-08-07T01:34:05.162665478Z level=info msg="Request Completed" method=GET path=/api/live/ws status=-1 remote_addr=177.23.89.224 time_ms=29 duration=29.703368ms size=0 referer= handler=/api/live/ws
logger=context userId=1 orgId=1 uname=admin t=2025-08-07T01:36:26.401389049Z level=info msg="Request Completed" method=GET path=/api/live/ws status=-1 remote_addr=177.69.164.161 time_ms=2 duration=2.350721ms size=0 referer= handler=/api/live/ws
logger=context userId=1 orgId=1 uname=admin t=2025-08-07T11:01:59.254541941Z level=info msg="Request Completed" method=GET path=/api/live/ws status=-1 remote_addr=177.23.89.224 time_ms=34 duration=34.482827ms size=0 referer= handler=/api/live/ws
logger=context userId=1 orgId=1 uname=admin t=2025-08-07T11:06:45.823482981Z level=info msg="Request Completed" method=GET path=/api/live/ws status=-1 remote_addr=177.69.164.161 time_ms=2 duration=2.602487ms size=0 referer= handler=/api/live/ws
[pmm@pmm-0 opt] # sqlite3 /srv/grafana/grafana.db ".dump user"
bash: sqlite3: command not found
[pmm@pmm-0 opt] # psql --version
psql (PostgreSQL) 14.18 - Percona Distribution
[pmm@pmm-0 opt] # ls /usr/bin/psql
/usr/bin/psql
[pmm@pmm-0 opt] # psql -U postgres -l
psql: error: connection to server on socket "/run/postgresql/.s.PGSQL.5432" failed: No such file or directory
	Is the server running locally and accepting connections on that socket?
[pmm@pmm-0 opt] # psql -U pmm -d pmm-managed
psql: error: connection to server on socket "/run/postgresql/.s.PGSQL.5432" failed: No such file or directory
	Is the server running locally and accepting connections on that socket?
[pmm@pmm-0 opt] # ls -lah /srv/lib/postgresql/
ls: cannot access '/srv/lib/postgresql/': No such file or directory
[pmm@pmm-0 opt] # find / -name PG_VERSION 2>/dev/null
/srv/postgres14/base/16409/PG_VERSION
/srv/postgres14/base/1/PG_VERSION
/srv/postgres14/base/13806/PG_VERSION
/srv/postgres14/base/13805/PG_VERSION
/srv/postgres14/base/16773/PG_VERSION
/srv/postgres14/PG_VERSION
[pmm@pmm-0 opt] # pg_ctl -D /srv/postgres14 start
bash: pg_ctl: command not found
[pmm@pmm-0 opt] # postgres -D /srv/postgres14
bash: postgres: command not found
[pmm@pmm-0 opt] # exit
exit
command terminated with exit code 127
adam.rodrigues@ADAMPC:~$ helm uninstall pmm -n observability
release "pmm" uninstalled
adam.rodrigues@ADAMPC:~$ kubectl delete pvc pmm-storage-pmm-0 -n observability
persistentvolumeclaim "pmm-storage-pmm-0" deleted
adam.rodrigues@ADAMPC:~$ helm install pmm percona/pmm -n observability
Error: INSTALLATION FAILED: Unable to continue with install: ***** "pmm-*****" in namespace "observability" exists and cannot be imported into the current release: invalid ownership metadata; label validation error: missing key "app.kubernetes.io/managed-by": must be set to "Helm"; annotation validation error: missing key "meta.helm.sh/release-name": must be set to "pmm"; annotation validation error: missing key "meta.helm.sh/release-namespace": must be set to "observability"
adam.rodrigues@ADAMPC:~$ kubectl delete ***** pmm-***** -n observability
***** "pmm-*****" deleted
adam.rodrigues@ADAMPC:~$ helm install pmm percona/pmm -n observability
NAME: pmm
LAST DEPLOYED: Thu Aug  7 12:12:17 2025
NAMESPACE: observability
STATUS: deployed
REVISION: 1
TEST SUITE: None
NOTES:
Percona Monitoring and Management (PMM)

An open source database monitoring, observability and management tool
Check more info here: https://docs.percona.com/percona-monitoring-and-management/index.html

adam.rodrigues@ADAMPC:~$ kubectl get pods -n observability -w
NAME    READY   STATUS    RESTARTS   AGE
pmm-0   0/1     Running   0          81s
pmm-0   1/1     Running   0          82s

adam.rodrigues@NXTN245:~$ kubectl exec -it pmm-0 -n observability -- bash
[pmm@pmm-0 opt] # change-admin-password admin
INFO [08-07|16:16:32] Starting Grafana                         logger=settings version=11.6.1 commit=11.6.1 branch=main compiled=2025-05-22T00:00:00Z
INFO [08-07|16:16:32] Config loaded from                       logger=settings file=/usr/share/grafana/conf/defaults.ini
INFO [08-07|16:16:32] Config loaded from                       logger=settings file=/etc/grafana/grafana.ini
INFO [08-07|16:16:32] Config overridden from Environment variable logger=settings var="GF_SECURITY_ADMIN_PASSWORD=*********"
INFO [08-07|16:16:32] Target                                   logger=settings target=[all]
INFO [08-07|16:16:32] Path Home                                logger=settings path=/usr/share/grafana
INFO [08-07|16:16:32] Path Data                                logger=settings path=/srv/grafana
INFO [08-07|16:16:32] Path Logs                                logger=settings path=/srv/logs
INFO [08-07|16:16:32] Path Plugins                             logger=settings path=/srv/grafana/plugins
INFO [08-07|16:16:32] Path Provisioning                        logger=settings path=/usr/share/grafana/conf/provisioning
INFO [08-07|16:16:32] App mode production                      logger=settings
INFO [08-07|16:16:32] FeatureToggles                           logger=featuremgmt savedItems=true prometheusAzureOverrideAudience=true alertingInsights=true transformationsRedesign=true alertingNotificationsStepMode=true awsAsyncQueryCaching=true recoveryThreshold=true alertingUIOptimizeReducer=true formatString=true alertingQueryAndExpressionsStepMode=true alertingNoDataErrorExecution=true dashgpt=true azureMonitorEnableUserAuth=true groupToNestedTableTransformation=true exploreMetrics=true recordedQueriesMulti=true lokiStructuredMetadata=true annotationPermissionUpdate=true panelMonitoring=true newPDFRendering=true cloudWatchCrossAccountQuerying=true preinstallAutoUpdate=true lokiQueryHints=true logsContextDatasourceUi=true newDashboardSharingComponent=true dataplaneFrontendFallback=true dashboardSceneForViewers=true tlsMemcached=true alertingSimplifiedRouting=true panelTitleSearch=true lokiLabelNamesQueryApi=true addFieldFromCalculationStatFunctions=true unifiedRequestLog=true cloudWatchDynamicLabels=true reportingUseRawTimeRange=true lokiQueryBuilder=true publicDashboardsScene=true dashboardSceneSolo=true logRowsPopoverMenu=true influxdbBackendMigration=true nestedFolders=true ssoSettingsSAML=true correlations=true userStorageAPI=true promQueryBuilder=true accessActionSets=true useSessionStorageForRedirection=true newFiltersUI=true alertingApiServer=true commandPalette=true logsExploreTableVisualisation=true kubernetesPlaylists=true explore2Dashboard=true alertRuleRestore=true cloudWatchNewLabelParsing=true lokiQuerySplitting=true pinNavItems=true alertingRuleVersionHistoryRestore=true cloudWatchRoundUpEndTime=true promQLScope=true ssoSettingsApi=true logsInfiniteScrolling=true dashboardScene=true
INFO [08-07|16:16:32] Connecting to DB                         logger=sqlstore dbtype=postgres
INFO [08-07|16:16:32] Locking database                         logger=migrator
INFO [08-07|16:16:32] Starting DB migrations                   logger=migrator
INFO [08-07|16:16:32] migrations completed                     logger=migrator performed=0 skipped=644 duration=487.196µs
INFO [08-07|16:16:32] Unlocking database                       logger=migrator
INFO [08-07|16:16:32] Envelope encryption state                logger=*****s enabled=true current provider=*****Key.v1

Admin password changed successfully ✔

[pmm@pmm-0 opt] # command terminated with exit code 137
